Добрый день


Высылаю готовое задание:

1) TweetTable.sql - скрипт создания первичной таблицы
2) NormalTables.sql - скрипт создания нормализованных таблиц и также колонки tweet_sentiment
3) test.py - программа на Python которая множен загружать данные в БД (БД должна быть создана заранее) и также обновлять поле tweet_sentiment на основании анализа уже загруженных данных
4) show_sentimental_Info.sql - скрипт, который выводит наиболее и наименее счастливую страну, локацию и пользователя
5) test.db - полученная БД

Краткое описание программы:
---------------------------
Программа принимает параметры 
test.py -d <database> -o <operation> -t <tweet_file> -w <words_file> -r

пример:
-d C:\mts\test.db -o calculate -t C:\mts\Documents_\three_minutes_tweets.json.txt -w C:\mts\Documents_\AFINN-111.txt 

описание параметров
-d - ПУТЬ К БД
-o = calculate      - проставляем поле tweet_sentiment на основе загруженных данных
       load              - грузим данные из файла в БД
-t путь к выгрузки твиттера
-w путь к файлу со словами
-r (опционально) если НЕ передано, то данные берутся напрямую из корня json, если передано, то грузятся только отдельными записями сообщения из под тэга "retweeted_status"
    Так как в выгрузке из корневого элемента twitter большая часть полей country_code и display_url пустые, попробовал выгрузить из retweeted_status - там больше этих значений заполнено

Возникли сложности с пониманием откуда именно брать поля:
name, tweet_text, country_code, display_url, lang, created_at, location
в итоге взял из:

для режима -r
	root(twitter object)->retweeted_status->user->name
	root(twitter object)->retweeted_status->user->location
	root(twitter object)->retweeted_status->place->country_code
	root(twitter object)->retweeted_status->text
	root(twitter object)->retweeted_status->lang
	root(twitter object)->retweeted_status->entities->media->display_url
	root(twitter object)->retweeted_status->created_at

для режима без -r
	root(twitter object)->user->name
	root(twitter object)->user->location
	root(twitter object)->place->country_code
	root(twitter object)->text
	root(twitter object)->lang
	root(twitter object)->entities->media->display_url
	root(twitter object)->created_at

	
По соотношениям - как я понял везде 1 ко 1, кроме display_url их может быть много (много media, много entities на 1 сообщение)

Тут я могу ошибаться, так как структуру twitter знаю не очень, брал описание отсюда
https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object

Смотрел формат в приличном виде через 
https://jsonformatter.curiousconcept.com/

По складыванию в денормализованную таблицу
-------------------------------------------
Из за того что display_url может быть более чем 1 строки в таблице tweet дублируются исходя из количеств display_url

-------------------------------------------
На счет расчета "среднего sentiment": тут возможно двоякое толкование, я сделал как сумму sentiment для всех слов:
Пример, если сообщение было "worrying wooo wooo" то я считаю как -3 + 4 + 4 = 5

PS: Это мой первый код на Python, поэтому не считаю его идеальным многое можно улучшить (например поиск количеств повторений слов)
    Также, решение сделано именно под тестовое количество данных (то что присылали) - если там миллиарды строк будет скорее всего подход будет другой

6) Все скрипты не рассчитаны на повторный вызов, проверки на наличия уже вставленных данных нет
   Для этого можно сохранять twitter_id и не добавлять те записи где уже он в БД
   То же самое касается нормализованных таблиц - справочников - по идеи там нужно писать merge команду чтобы каждый раз обновлялось
   
7)
 "6. Подумать как можно организовать процесс ежедневной оценки параметров в п. 5 (Production решение) 
  и описать из каких шагов ETL будет состоять процесс трансформации до получения и хранения конечной информации
  Первый этап: данные приходят на FTP (tweet.json+AFINN.txt), последний этап: данные отгружаются на другой FTP"   
  
  Тут приходит на ум вопрос: заточена ли система ТОЛЬКО под решение под вывод "наиболее и наименее счастливой страны, локации и пользователя"?

  Если да то, то нам нет необходимости хранить все данные по всем tweetам, а каждый раз считывая новый файл обновлять 1 табличку с 6 полями:
  (город_мин, город_макс, локация_мин, локация_макс, пользователь_мин, пользователь_макс)
  Предыдущая детализация не будет нужна.
  
  Алгоритм может быть такой:
  1) данные приходят на FTP (tweet.json+AFINN.txt) - тут нужно договориться чтобы файлы имели уникальное наименование например с датой + какой-то числовой индекс если в течении секунды несколько приходит
  2) система запускает ETL в однопоточном или (многопоточном режиме) 
  3) читает список файлов с FTP с смотрит какие из них НЕ были обработаны (мы на БД ведем таблицу с именами обработанных файлов)
  4) берем первый попавшийся НЕОБРАБОТАННЫЙ и помечаем в бд статусом что НАЧАЛИ обрабатывать
  5) заполняем все нормализованные таблицы данными из этого файла (все что делал я в этой задаче скриптом, лучше делать на лету в ETL)
  6) после успешно завершения меняем статус этого файла на ОБРАБОТАННЫЙ
  7) продолжаем обрабатывать файлы пока все не обработаем 
     (тут важный момент - файлы могут валиться с большой скоростью, поэтому в этом случае на этапе 3 считать все доступные файлы для обработки и обработать ТОЛЬКО их - остальные на след итерации)
  8) рассчитываем результат 	"наиболее и наименее счастливой страны, локации и пользователя" складываем в таблицу (здесь можно сменить статус файла на РАСЧИТАНО)
  9) формируем файл (json, xml) с этими результатами и отсылаем на новый FTP (здесь можно сменить статус файла на ОТОСЛАНО)
  10)как вариант полностью ОБРАБОТАННЫЕ и ОТОСЛАННЫЕ файлы можно переместить в другую папку на FTP (или удалить) чтобы процесс чтение файлов не подвисал из-за миллионов уже обработанных файлов

Статусы пригодятся чтобы отслеживать состояние системы. 

 PS: с AFINN.txt тоже можно по разному, можно держать на БД последнюю копию справочника, можно использовать всегда последнюю.